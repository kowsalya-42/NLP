{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.Tokenization of text**"
      ],
      "metadata": {
        "id": "1QkXyJQAoFIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus = \"\"\"This is an exciting time to be working in speech and language processing.\n",
        "Historically distinct fields (natural language processing, speech recognition,\n",
        "computational linguistics, computational psycholinguistics) have begun to merge.\"\"\"\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "print(\"Original corpus:\\n\",corpus,\"\\n\")\n",
        "print(\"Tokenized words : \\n\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7HjDZ9noPLt",
        "outputId": "89b588ae-0a52-47c3-94f8-3dfe63685b5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original corpus:\n",
            " This is an exciting time to be working in speech and language processing.\n",
            "Historically distinct fields (natural language processing, speech recognition,\n",
            "computational linguistics, computational psycholinguistics) have begun to merge. \n",
            "\n",
            "Tokenized words : \n",
            " ['This', 'is', 'an', 'exciting', 'time', 'to', 'be', 'working', 'in', 'speech', 'and', 'language', 'processing', '.', 'Historically', 'distinct', 'fields', '(', 'natural', 'language', 'processing', ',', 'speech', 'recognition', ',', 'computational', 'linguistics', ',', 'computational', 'psycholinguistics', ')', 'have', 'begun', 'to', 'merge', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Stop word Removal**"
      ],
      "metadata": {
        "id": "GVTQhf7ppJB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "corpus = \"\"\"This is an exciting time to be working in speech and language processing.\n",
        "Historically distinct fields (natural language processing, speech recognition, computational\n",
        "linguistics, computational psycholinguistics) have begun to merge.\"\"\"\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "print(\"Original corpus :\\n\",corpus,\"\\n\")\n",
        "print(\"Tokenized words : \\n\", tokens)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "rel_words = [rel for rel in tokens if not rel in stop_words]\n",
        "print(\"\\nTokens without stop words :\\n\",rel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS20p1lQoQ5G",
        "outputId": "ea2a993d-805a-4fff-c330-0183b2245e57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original corpus :\n",
            " This is an exciting time to be working in speech and language processing.\n",
            "Historically distinct fields (natural language processing, speech recognition, computational\n",
            "linguistics, computational psycholinguistics) have begun to merge. \n",
            "\n",
            "Tokenized words : \n",
            " ['This', 'is', 'an', 'exciting', 'time', 'to', 'be', 'working', 'in', 'speech', 'and', 'language', 'processing', '.', 'Historically', 'distinct', 'fields', '(', 'natural', 'language', 'processing', ',', 'speech', 'recognition', ',', 'computational', 'linguistics', ',', 'computational', 'psycholinguistics', ')', 'have', 'begun', 'to', 'merge', '.']\n",
            "\n",
            "Tokens without stop words :\n",
            " ['This', 'exciting', 'time', 'working', 'speech', 'language', 'processing', '.', 'Historically', 'distinct', 'fields', '(', 'natural', 'language', 'processing', ',', 'speech', 'recognition', ',', 'computational', 'linguistics', ',', 'computational', 'psycholinguistics', ')', 'begun', 'merge', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Stemming of text**"
      ],
      "metadata": {
        "id": "CXE1d-jBph0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = \"\"\"This is an exciting time to be working in speech and language processing.\n",
        "Historically distinct fields (natural language processing, speech recognition, computational\n",
        "linguistics, computational psycholinguistics) have begun to merge.\"\"\"\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "print(\"Original corpus:\\n\",corpus,\"\\n\")\n",
        "print(\"Tokenized words : \\n\", tokens)\n",
        "porter = PorterStemmer()\n",
        "stem_words = [porter.stem(stem) for stem in tokens]\n",
        "print(\"\\nStemmed words :\\n\",stem_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpFHbekspOqn",
        "outputId": "808f57b5-5ba8-4cb6-d11a-ec8bcf635053"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original corpus:\n",
            " This is an exciting time to be working in speech and language processing.\n",
            "Historically distinct fields (natural language processing, speech recognition, computational\n",
            "linguistics, computational psycholinguistics) have begun to merge. \n",
            "\n",
            "Tokenized words : \n",
            " ['This', 'is', 'an', 'exciting', 'time', 'to', 'be', 'working', 'in', 'speech', 'and', 'language', 'processing', '.', 'Historically', 'distinct', 'fields', '(', 'natural', 'language', 'processing', ',', 'speech', 'recognition', ',', 'computational', 'linguistics', ',', 'computational', 'psycholinguistics', ')', 'have', 'begun', 'to', 'merge', '.']\n",
            "\n",
            "Stemmed words :\n",
            " ['thi', 'is', 'an', 'excit', 'time', 'to', 'be', 'work', 'in', 'speech', 'and', 'languag', 'process', '.', 'histor', 'distinct', 'field', '(', 'natur', 'languag', 'process', ',', 'speech', 'recognit', ',', 'comput', 'linguist', ',', 'comput', 'psycholinguist', ')', 'have', 'begun', 'to', 'merg', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Lemmatization**"
      ],
      "metadata": {
        "id": "kb0BBgdJpqWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "corpus = \"studies studying cries cry\"\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "print(\"Original corpus:\\n\",corpus,\"\\n\")\n",
        "print(\"Tokenized words : \\n\", tokens)\n",
        "lemma = WordNetLemmatizer()\n",
        "lem_words = [lemma.lemmatize(lem) for lem in tokens]\n",
        "print(\"\\nLemmatized words :\\n\",lem_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_45L1kupmuE",
        "outputId": "7b03c4f5-256b-4d0f-fcad-ded8992c091b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original corpus:\n",
            " studies studying cries cry \n",
            "\n",
            "Tokenized words : \n",
            " ['studies', 'studying', 'cries', 'cry']\n",
            "\n",
            "Lemmatized words :\n",
            " ['study', 'studying', 'cry', 'cry']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.N-gram model**"
      ],
      "metadata": {
        "id": "Df23Gu2kpz_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import reuters\n",
        "from nltk.util import trigrams\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "print(dict(model[('the','bank')]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or7jKhz6ptzr",
        "outputId": "a67bf4ce-5f19-4537-e706-b910e4fdd997"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'was': 0.033707865168539325, ',': 0.018726591760299626, \"'\": 0.29213483146067415, 'holding': 0.018726591760299626, '.': 0.026217228464419477, 'credit': 0.003745318352059925, 'said': 0.15355805243445692, 'has': 0.026217228464419477, 'bought': 0.00749063670411985, 'reported': 0.003745318352059925, 'would': 0.0299625468164794, 'regards': 0.00749063670411985, 'operating': 0.003745318352059925, 'as': 0.003745318352059925, 'will': 0.018726591760299626, 'is': 0.011235955056179775, 'loaned': 0.003745318352059925, 'had': 0.0299625468164794, 'to': 0.0449438202247191, 'lending': 0.011235955056179775, 'by': 0.00749063670411985, '\"': 0.003745318352059925, 'of': 0.003745318352059925, 'when': 0.003745318352059925, 'were': 0.003745318352059925, 'dropped': 0.003745318352059925, 'must': 0.003745318352059925, 'research': 0.003745318352059925, 'group': 0.00749063670411985, 'card': 0.003745318352059925, 'did': 0.003745318352059925, 'still': 0.011235955056179775, 'and': 0.00749063670411985, 'in': 0.003745318352059925, 'board': 0.018726591760299626, 'total': 0.003745318352059925, 'recorded': 0.003745318352059925, 'wrote': 0.003745318352059925, 'left': 0.003745318352059925, 'wants': 0.003745318352059925, 'added': 0.003745318352059925, 'announced': 0.00749063670411985, 'trimmed': 0.003745318352059925, 'flexibility': 0.003745318352059925, 'forecast': 0.00749063670411985, 'syndications': 0.003745318352059925, 'advisory': 0.003745318352059925, 'posted': 0.003745318352059925, 'previously': 0.003745318352059925, 'nearly': 0.003745318352059925, 'today': 0.003745318352059925, 'employees': 0.00749063670411985, 'official': 0.00749063670411985, 'estimated': 0.00749063670411985, 'planned': 0.003745318352059925, 'lost': 0.003745318352059925, '-': 0.003745318352059925, 'expected': 0.003745318352059925, 'statement': 0.003745318352059925, 'this': 0.003745318352059925, 'tomorrow': 0.00749063670411985, 'itself': 0.00749063670411985, 'financing': 0.003745318352059925, 'seemed': 0.00749063670411985, 'on': 0.003745318352059925, 'prime': 0.003745318352059925, 'strike': 0.003745318352059925, 'buys': 0.003745318352059925, 'spokesman': 0.003745318352059925, 'earned': 0.003745318352059925}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.POS tagging.**"
      ],
      "metadata": {
        "id": "_w4NeEIAqJXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Ensure the specific English tagger resource is downloaded\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"\"\"This is an exciting time to be working in speech and language processing. Historically\n",
        "distinct fields (natural language processing, speech recognition, computational linguistics,\n",
        "computational psycholinguistics) have begun to merge.\"\"\"\n",
        "\n",
        "tokens = text.split(); print(\"Tokenized words :\\n\",tokens)\n",
        "tagged_words = pos_tag(tokens)\n",
        "print(\"\\nPOS tagged words : \\n\",tagged_words)\n",
        "# Removed the 'error' at the end as it was causing a SyntaxError"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbL1EhPHp6Ck",
        "outputId": "0782f42b-239d-4ac4-be33-c10fc1481374"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words :\n",
            " ['This', 'is', 'an', 'exciting', 'time', 'to', 'be', 'working', 'in', 'speech', 'and', 'language', 'processing.', 'Historically', 'distinct', 'fields', '(natural', 'language', 'processing,', 'speech', 'recognition,', 'computational', 'linguistics,', 'computational', 'psycholinguistics)', 'have', 'begun', 'to', 'merge.']\n",
            "\n",
            "POS tagged words : \n",
            " [('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('exciting', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('be', 'VB'), ('working', 'VBG'), ('in', 'IN'), ('speech', 'NN'), ('and', 'CC'), ('language', 'NN'), ('processing.', 'NN'), ('Historically', 'NNP'), ('distinct', 'JJ'), ('fields', 'NNS'), ('(natural', 'JJ'), ('language', 'NN'), ('processing,', 'NN'), ('speech', 'NN'), ('recognition,', 'VBP'), ('computational', 'JJ'), ('linguistics,', 'JJ'), ('computational', 'JJ'), ('psycholinguistics)', 'NN'), ('have', 'VBP'), ('begun', 'VBN'), ('to', 'TO'), ('merge.', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Chunking**"
      ],
      "metadata": {
        "id": "ymGzCjioqpF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "corpus =\"\"\"\"This is an exciting time to be working in speech and language processing. Historically\n",
        "distinct fields (natural language processing, speech recognition, computational linguistics,\n",
        "computational psycholinguistics) have begun to merge.\"\"\"\n",
        "tokens = corpus.split()\n",
        "print(\"Original corpus :\\n\",corpus)\n",
        "print(\"\\nSplit Text :\\n\",tokens)\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tokens_tag = pos_tag(tokens)\n",
        "print(\"\\nPOS tagging :\\n\",tokens_tag)\n",
        "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"\\nAfter Regex :\\n\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"\\nChunked Text \\n\",output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQz4NJsPqOdU",
        "outputId": "d37c9a0f-226e-4e63-dc11-5c206ee8c79e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original corpus :\n",
            " \"This is an exciting time to be working in speech and language processing. Historically\n",
            "distinct fields (natural language processing, speech recognition, computational linguistics,\n",
            "computational psycholinguistics) have begun to merge.\n",
            "\n",
            "Split Text :\n",
            " ['\"This', 'is', 'an', 'exciting', 'time', 'to', 'be', 'working', 'in', 'speech', 'and', 'language', 'processing.', 'Historically', 'distinct', 'fields', '(natural', 'language', 'processing,', 'speech', 'recognition,', 'computational', 'linguistics,', 'computational', 'psycholinguistics)', 'have', 'begun', 'to', 'merge.']\n",
            "\n",
            "POS tagging :\n",
            " [('\"This', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('exciting', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('be', 'VB'), ('working', 'VBG'), ('in', 'IN'), ('speech', 'NN'), ('and', 'CC'), ('language', 'NN'), ('processing.', 'NN'), ('Historically', 'NNP'), ('distinct', 'JJ'), ('fields', 'NNS'), ('(natural', 'JJ'), ('language', 'NN'), ('processing,', 'NN'), ('speech', 'NN'), ('recognition,', 'VBP'), ('computational', 'JJ'), ('linguistics,', 'JJ'), ('computational', 'JJ'), ('psycholinguistics)', 'NN'), ('have', 'VBP'), ('begun', 'VBN'), ('to', 'TO'), ('merge.', 'VB')]\n",
            "\n",
            "After Regex :\n",
            " chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
            "\n",
            "Chunked Text \n",
            " (S\n",
            "  (mychunk \"This/NN)\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  (mychunk exciting/JJ)\n",
            "  (mychunk time/NN)\n",
            "  to/TO\n",
            "  be/VB\n",
            "  working/VBG\n",
            "  in/IN\n",
            "  (mychunk speech/NN and/CC)\n",
            "  (mychunk language/NN processing./NN Historically/NNP distinct/JJ)\n",
            "  (mychunk fields/NNS (natural/JJ)\n",
            "  (mychunk language/NN processing,/NN speech/NN)\n",
            "  recognition,/VBP\n",
            "  (mychunk computational/JJ linguistics,/JJ computational/JJ)\n",
            "  (mychunk psycholinguistics)/NN)\n",
            "  have/VBP\n",
            "  begun/VBN\n",
            "  to/TO\n",
            "  merge./VB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.Named Entity Recognition**"
      ],
      "metadata": {
        "id": "yO5C0iSHq6Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"\"\"Sundar Pichai is the CEO of Google. Its headquarter is in Mountain View.Yuvraj Singh revealed\n",
        "the key advice Sachin Tendulkar gave during the 2011 World Cup which helped the team immensely.\n",
        "Tendulkar was India's highest run-getter during the\"\"\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHToR2TQquqA",
        "outputId": "f47f260e-55d6-48a1-ac76-72b0a9b5245f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sundar Pichai PERSON\n",
            "Google ORG\n",
            "Mountain View GPE\n",
            "Yuvraj Singh PERSON\n",
            "Sachin Tendulkar PERSON\n",
            "the 2011 World Cup EVENT\n",
            "Tendulkar PERSON\n",
            "India GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.Bag of Words**"
      ],
      "metadata": {
        "id": "2Uvk7oGsrJax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import heapq\n",
        "nltk.download('punkt')\n",
        "\n",
        "text=\"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And\n",
        "they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that\n",
        "outstanding introduction. I have a bunch of good friends here today, including somebody who I served\n",
        "with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick\n",
        "Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time,\n",
        "and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President\n",
        "Killeen and everybody at the U of I System for making it possible for me to be here today. And I am\n",
        "deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for\n",
        "so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the\n",
        "room. I know people are still wondering why I didn’t speak at the commencement.\"\"\"\n",
        "\n",
        "dataset = nltk.sent_tokenize(text)\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset[i] = dataset[i].lower()\n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
        "\n",
        "word2count = {}\n",
        "\n",
        "for data in dataset:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
        "print(freq_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUhVVJ-trBRz",
        "outputId": "32c75fb7-fc6c-4837-9162-57a124eb86cb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'the', 'to', 'and', 'in', 'for', 'here', 'that', 'at', 'who', 'is', 'somebody', 's', 'it', 'have', 'of', 'beans', 'we', 'were', 'outstanding', 'a', 'today', 'by', 'governor', 't', 'he', 'want', 'me', 'was', 'trying', 'explain', 'as', 'flying', 'corn', 'they', 'very', 'impressed', 'my', 'agricultural', 'knowledge', 'please', 'give', 'up', 'amaury', 'once', 'again', 'introduction', 'bunch', 'good', 'friends', 'including', 'served', 'with', 'one', 'finest', 'senators', 'country', 're', 'lucky', 'him', 'your', 'senator', 'dick', 'durbin', 'also', 'noticed', 'way', 'former', 'edgar', 'haven', 'seen', 'long', 'time', 'somehow', 'has', 'not', 'aged', 'great', 'see', 'you', 'thank', 'president', 'killeen', 'everybody', 'u', 'system', 'making', 'possible', 'be', 'am', 'deeply', 'honored', 'paul', 'douglas', 'award', 'being', 'given', 'set', 'path', 'so']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.N-Gram Models.**"
      ],
      "metadata": {
        "id": "aAdD2vwarVpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "word_data = \"The best performance can bring in sky high success.\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print(list(nltk.bigrams(nltk_tokens)))\n",
        "print(list(nltk.trigrams(nltk_tokens)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGUMfARYrP2G",
        "outputId": "c5ad2a8b-58f0-4d57-dee4-3f2b4f9f58ca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'best'), ('best', 'performance'), ('performance', 'can'), ('can', 'bring'), ('bring', 'in'), ('in', 'sky'), ('sky', 'high'), ('high', 'success'), ('success', '.')]\n",
            "[('The', 'best', 'performance'), ('best', 'performance', 'can'), ('performance', 'can', 'bring'), ('can', 'bring', 'in'), ('bring', 'in', 'sky'), ('in', 'sky', 'high'), ('sky', 'high', 'success'), ('high', 'success', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qi_CIqYjra8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
